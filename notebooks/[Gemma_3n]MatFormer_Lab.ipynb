{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "04Ug2C7GHUNO"
      },
      "outputs": [],
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ieA4poCNHdSY"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVCGxEikHgUc"
      },
      "source": [
        "# MatFormer Lab:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOG0dm7LHmBE"
      },
      "source": [
        " <table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Gemma/[Gemma_3n]MatFormer_Lab.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o6A5O7ofZTH"
      },
      "source": [
        "Gemma 3n is a multimodal, multilingual model from the Gemma family of models. You can read about Gemma 3n in [its docs](https://ai.google.dev/gemma/docs/gemma-3n) and [launch blog post](https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide). It is a unique model that is natively elastic, which means you have nested models! Gemma 3n was trained as a E4B model (effectively loads 4B parameters) with 35 layers and a 16,384 FFN hidden dimension. It has a E2B (30 layers and 8,192 FFN hidden dimension) nested within it and jointly trained as a MatFormer.\n",
        "\n",
        "[MatFormer](https://arxiv.org/abs/2310.07707) (🪆Matryoshka Transformer) architecture is a novel nested transformer built for elastic inference. Think of it like Matryoshka dolls: a larger model contains smaller, fully functional versions of itself. This approach extends the concept of [Matryoshka Representation Learning](https://huggingface.co/papers/2205.13147) from just embeddings to all transformer components.\n",
        "\n",
        "Saving memory by having E2B nested within E4B is practically useful, but what makes MatFormer so powerful is its capability to smoothly span the entire Pareto-optimal accuracy-vs-model size cruve between E2B and E4B without any additional training. Using a simple technique called **Mix-n-Match** one can extract a model of any size between E2B and E4B from the main E4B model.\n",
        "\n",
        "Why would you slice a model? Given your specific deployment requirements, the E2B and E4B may not be the right fit. You may want to get an E3B, for example, which has quality higher than the E2B while requiring less compute than E4B.\n",
        "\n",
        "**In this notebook, you get to play with MatFormers and Mix-n-Match. You will specify which configuration you want for the submodel based on FFN dimension and skp layers, and then you will export the model to Hugging Face, enabling you to use with your favorite tools.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys5kfGnpHsDV"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBXCVVERIGzM",
        "outputId": "105680d1-ad74-42d8-bace-b5b0522c69ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers>=4.53 in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (4.54.1)\n",
            "Requirement already satisfied: timm>=1.0.16 in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (1.0.19)\n",
            "Requirement already satisfied: filelock in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from transformers>=4.53) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from transformers>=4.53) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from transformers>=4.53) (2.0.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from transformers>=4.53) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from transformers>=4.53) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from transformers>=4.53) (2025.7.34)\n",
            "Requirement already satisfied: requests in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from transformers>=4.53) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from transformers>=4.53) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from transformers>=4.53) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from transformers>=4.53) (4.66.5)\n",
            "Requirement already satisfied: torch in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from timm>=1.0.16) (2.5.0)\n",
            "Requirement already satisfied: torchvision in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from timm>=1.0.16) (0.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.53) (2025.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.53) (4.11.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from tqdm>=4.27->transformers>=4.53) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from requests->transformers>=4.53) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from requests->transformers>=4.53) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from requests->transformers>=4.53) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from requests->transformers>=4.53) (2024.8.30)\n",
            "Requirement already satisfied: networkx in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from torch->timm>=1.0.16) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from torch->timm>=1.0.16) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from torch->timm>=1.0.16) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from sympy==1.13.1->torch->timm>=1.0.16) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from torchvision->timm>=1.0.16) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages (from jinja2->torch->timm>=1.0.16) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "# @title Install dependencies\n",
        "# @markdown Run this cell to install all the required dependencies. In particular, you'll need Hugging Face `transformers` and `timm` versions that support Gemma 3n. Note that you may need to restart the notebook after executing the following cell.\n",
        "\n",
        "# Install a transformers version that supports Gemma 3n (>= 4.53)\n",
        "!pip install \"transformers>=4.53\" \"timm>=1.0.16\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "bcb3b9b0a6a0405d9394e9859c2cabc4",
            "24c22a54b2f048afade6447b4fe77d0b",
            "036afaa2256f446581ef1559d15b555b",
            "68224fc54d0840b7a45af5874d7513d3",
            "d3ba4b3101de4827a4e58dcc1add1d85",
            "c3c72e8c65c047e39431efed6889ecdb",
            "70c2df2c30f04e8088111033e6956b6d",
            "06098a3904784a2ba20cb6689419d525",
            "b8f2ecb8c7b144529b357ab2d243fd6c",
            "9fcb4212ff934c7084913cec261997f6",
            "90e0021b763041309cf7a620160b488c",
            "366cbe589a2b465cbd2a4538060c6b5c",
            "922dd434870e4fb69e94689112e61c3f",
            "a71c0fbff91d4402ae04c52e4e4447ca",
            "6df786f15e6547b79375cdc5d831e634",
            "607aea53773d4220b6137b9b255deadd",
            "0dae1b6657f04794ac9a0f5265764c9c",
            "9b7962104b134d618dfd9d8d9a217696",
            "7bd5626e14a54280bc6ec225f11cfe6c",
            "68661aaf7e2441139835d23d80262d0a"
          ]
        },
        "id": "GFh8AhruHeMl",
        "outputId": "14e55e6a-9f7b-441c-a5d4-2b477d1b83a4"
      },
      "outputs": [],
      "source": [
        "# @title Login to Hugging Face\n",
        "# @markdown This is required so you can push the model to Hugging Face. You also need to make sure you have access to the Gemma 3n model repositories.\n",
        "\n",
        "from huggingface_hub import login\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "login(token=os.getenv(\"HUGGINGFACE_HUB_TOKEN\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yKFl4n5cLF9J"
      },
      "outputs": [],
      "source": [
        "# @title Import and Export Options\n",
        "# @markdown The MatFormer Lab allows you to load a Gemma 3n 4B checkpoint (either pre-trained or instruct-tuned) and to slice it. Below, please specify:\n",
        "\n",
        "# @markdown * The original repository ID from the checkpoint in Hugging Face\n",
        "\n",
        "# @markdown * A local path where the model will be saved\n",
        "\n",
        "# @markdown * A name of a repository to push the new checkpoint to\n",
        "\n",
        "original_model_id = \"google/gemma-3n-E4B-it\" # @param [\"google/gemma-3n-E4B-it\", \"google/gemma-3n-E4B-pt\"]\n",
        "local_output_path = \"my_modified_gemma_3n_model\" # @param {type:\"string\"}\n",
        "push_hf_repo_id = \"username/test-submodel\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P16x_lxGCcgc"
      },
      "source": [
        "### Slicing configuration\n",
        "\n",
        "As part of the Gemma 3n release, we share optimal slicing configurations as a [Hugging Face dataset repository](https://huggingface.co/datasets/google/gemma3n-slicing-configs), although you can also explore with your own configurations below.\n",
        "\n",
        "Each configuration specifies:\n",
        "* The hidden dimensions of the FFN\n",
        "* Which layers, if any, to skip\n",
        "* A MMLU accuracy associated with the pre-trained checkpoints\n",
        "\n",
        "`layer-level` and `block-level` configurations are a result of varying the hidden dimensions of the FFN either at a `layer-level` (fine-grained) or at a `block-level` (4 local + 1 global layers).\n",
        "\n",
        "At a `layer-level`, we found that having the global layers (instead of local layers) have higher capacity helps with accuracy for the same model size.\n",
        "\n",
        "At a `block-level`, we found that the block skipped for E2B (ie., layers 20-24) would benfit from higher capacity when not skipped and earlier blocks can work well with lower-capacity compared to the later blocks.\n",
        "\n",
        "**We invite the community to find even better configurations that lie on the Pareto-optimal curve between E2B and E4B!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "UHdEebcGYats",
        "outputId": "8b20d162-c3fd-4dcc-ad10-fe4c1cadbd33"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th># Layers</th>\n",
              "      <th># Effective Params (B)</th>\n",
              "      <th>MMLU PT accuracy</th>\n",
              "      <th>FFN Hidden Dims</th>\n",
              "      <th>Layers Skipped</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Main model</td>\n",
              "      <td>35</td>\n",
              "      <td>3.98</td>\n",
              "      <td>62.30%</td>\n",
              "      <td>[2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Config for official E2B Model</td>\n",
              "      <td>30</td>\n",
              "      <td>1.91</td>\n",
              "      <td>50.90%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>[20, 21, 22, 23, 24]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Config for E1.96B (layer-level)</td>\n",
              "      <td>30</td>\n",
              "      <td>1.96</td>\n",
              "      <td>53.40%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>[20, 21, 22, 23, 24]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Config for E2.54B (layer-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>2.54</td>\n",
              "      <td>55.40%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Config for E2.69B (layer-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>2.69</td>\n",
              "      <td>57.70%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Config for E2.98B (layer-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>2.98</td>\n",
              "      <td>59.50%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Config for E3.18B (layer-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>3.18</td>\n",
              "      <td>61.80%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Config for E3.39B (layer-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>3.39</td>\n",
              "      <td>63.00%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Config for E3.59B (layer-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>3.59</td>\n",
              "      <td>63.40%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Config for E3.79B (layer-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>3.79</td>\n",
              "      <td>63.40%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Config for E2.49B (block-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>2.49</td>\n",
              "      <td>54.50%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Config for E2.73B (block-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>2.73</td>\n",
              "      <td>57.10%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Config for E2.98B (block-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>2.98</td>\n",
              "      <td>59.50%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Config for E3.24B (block-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>3.24</td>\n",
              "      <td>60.70%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Config for E3.49B (block-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>3.49</td>\n",
              "      <td>61.40%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Config for E3.79B (block-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>3.74</td>\n",
              "      <td>62.00%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               name  # Layers  # Effective Params (B)  \\\n",
              "0                        Main model        35                    3.98   \n",
              "1     Config for official E2B Model        30                    1.91   \n",
              "2   Config for E1.96B (layer-level)        30                    1.96   \n",
              "3   Config for E2.54B (layer-level)        35                    2.54   \n",
              "4   Config for E2.69B (layer-level)        35                    2.69   \n",
              "5   Config for E2.98B (layer-level)        35                    2.98   \n",
              "6   Config for E3.18B (layer-level)        35                    3.18   \n",
              "7   Config for E3.39B (layer-level)        35                    3.39   \n",
              "8   Config for E3.59B (layer-level)        35                    3.59   \n",
              "9   Config for E3.79B (layer-level)        35                    3.79   \n",
              "10  Config for E2.49B (block-level)        35                    2.49   \n",
              "11  Config for E2.73B (block-level)        35                    2.73   \n",
              "12  Config for E2.98B (block-level)        35                    2.98   \n",
              "13  Config for E3.24B (block-level)        35                    3.24   \n",
              "14  Config for E3.49B (block-level)        35                    3.49   \n",
              "15  Config for E3.79B (block-level)        35                    3.74   \n",
              "\n",
              "   MMLU PT accuracy                                    FFN Hidden Dims  \\\n",
              "0            62.30%  [2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2...   \n",
              "1            50.90%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "2            53.40%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "3            55.40%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "4            57.70%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "5            59.50%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "6            61.80%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "7            63.00%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "8            63.40%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "9            63.40%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "10           54.50%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "11           57.10%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "12           59.50%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "13           60.70%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "14           61.40%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "15           62.00%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "\n",
              "          Layers Skipped  \n",
              "0                    NaN  \n",
              "1   [20, 21, 22, 23, 24]  \n",
              "2   [20, 21, 22, 23, 24]  \n",
              "3                    NaN  \n",
              "4                    NaN  \n",
              "5                    NaN  \n",
              "6                    NaN  \n",
              "7                    NaN  \n",
              "8                    NaN  \n",
              "9                    NaN  \n",
              "10                   NaN  \n",
              "11                   NaN  \n",
              "12                   NaN  \n",
              "13                   NaN  \n",
              "14                   NaN  \n",
              "15                   NaN  "
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"hf://datasets/google/gemma3n-slicing-configs/configs.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th># Layers</th>\n",
              "      <th># Effective Params (B)</th>\n",
              "      <th>MMLU PT accuracy</th>\n",
              "      <th>FFN Hidden Dims</th>\n",
              "      <th>Layers Skipped</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Main model</td>\n",
              "      <td>35</td>\n",
              "      <td>3.98</td>\n",
              "      <td>62.30%</td>\n",
              "      <td>[2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Config for E3.79B (layer-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>3.79</td>\n",
              "      <td>63.40%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Config for E3.79B (block-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>3.74</td>\n",
              "      <td>62.00%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Config for E3.59B (layer-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>3.59</td>\n",
              "      <td>63.40%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Config for E3.49B (block-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>3.49</td>\n",
              "      <td>61.40%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Config for E3.39B (layer-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>3.39</td>\n",
              "      <td>63.00%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Config for E3.24B (block-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>3.24</td>\n",
              "      <td>60.70%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Config for E3.18B (layer-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>3.18</td>\n",
              "      <td>61.80%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Config for E2.98B (layer-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>2.98</td>\n",
              "      <td>59.50%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Config for E2.98B (block-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>2.98</td>\n",
              "      <td>59.50%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Config for E2.73B (block-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>2.73</td>\n",
              "      <td>57.10%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Config for E2.69B (layer-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>2.69</td>\n",
              "      <td>57.70%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Config for E2.54B (layer-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>2.54</td>\n",
              "      <td>55.40%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Config for E2.49B (block-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>2.49</td>\n",
              "      <td>54.50%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Config for E1.96B (layer-level)</td>\n",
              "      <td>30</td>\n",
              "      <td>1.96</td>\n",
              "      <td>53.40%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>[20, 21, 22, 23, 24]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Config for official E2B Model</td>\n",
              "      <td>30</td>\n",
              "      <td>1.91</td>\n",
              "      <td>50.90%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>[20, 21, 22, 23, 24]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               name  # Layers  # Effective Params (B)  \\\n",
              "0                        Main model        35                    3.98   \n",
              "9   Config for E3.79B (layer-level)        35                    3.79   \n",
              "15  Config for E3.79B (block-level)        35                    3.74   \n",
              "8   Config for E3.59B (layer-level)        35                    3.59   \n",
              "14  Config for E3.49B (block-level)        35                    3.49   \n",
              "7   Config for E3.39B (layer-level)        35                    3.39   \n",
              "13  Config for E3.24B (block-level)        35                    3.24   \n",
              "6   Config for E3.18B (layer-level)        35                    3.18   \n",
              "5   Config for E2.98B (layer-level)        35                    2.98   \n",
              "12  Config for E2.98B (block-level)        35                    2.98   \n",
              "11  Config for E2.73B (block-level)        35                    2.73   \n",
              "4   Config for E2.69B (layer-level)        35                    2.69   \n",
              "3   Config for E2.54B (layer-level)        35                    2.54   \n",
              "10  Config for E2.49B (block-level)        35                    2.49   \n",
              "2   Config for E1.96B (layer-level)        30                    1.96   \n",
              "1     Config for official E2B Model        30                    1.91   \n",
              "\n",
              "   MMLU PT accuracy                                    FFN Hidden Dims  \\\n",
              "0            62.30%  [2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2...   \n",
              "9            63.40%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "15           62.00%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "8            63.40%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "14           61.40%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "7            63.00%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "13           60.70%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "6            61.80%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "5            59.50%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "12           59.50%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "11           57.10%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "4            57.70%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "3            55.40%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "10           54.50%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "2            53.40%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "1            50.90%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "\n",
              "          Layers Skipped  \n",
              "0                    NaN  \n",
              "9                    NaN  \n",
              "15                   NaN  \n",
              "8                    NaN  \n",
              "14                   NaN  \n",
              "7                    NaN  \n",
              "13                   NaN  \n",
              "6                    NaN  \n",
              "5                    NaN  \n",
              "12                   NaN  \n",
              "11                   NaN  \n",
              "4                    NaN  \n",
              "3                    NaN  \n",
              "10                   NaN  \n",
              "2   [20, 21, 22, 23, 24]  \n",
              "1   [20, 21, 22, 23, 24]  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ordene a coluna # Effective Params (B)\n",
        "df.sort_values(by=\"# Effective Params (B)\", ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldNQRKptGnbU"
      },
      "source": [
        "Based on your deployment scenarios, you may want to pick a different config. Select below your preferred one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BfNaTzifGa9x",
        "outputId": "2fe89d74-5370-42df-af90-1836b3fd90ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config for E2.98B (block-level)\n",
            "\n",
            "Layers Skipped:\n",
            "[]\n",
            "\n",
            "FFN Hidden Dims:\n",
            "('[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * '\n",
            " '4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * '\n",
            " '8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * '\n",
            " '8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 4, 2_048 * 4, 2_048 * '\n",
            " '4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * '\n",
            " '4]')\n"
          ]
        }
      ],
      "source": [
        "#@title Config details\n",
        "\n",
        "import ast\n",
        "from pprint import pprint\n",
        "\n",
        "config_name = \"Config for E2.98B (block-level)\" #\"Config for official E2B Model\"# @param ['Config for official E2B Model', 'Config for E1.96B (layer-level)', 'Config for E2.54B (layer-level)', 'Config for E2.69B (layer-level)', 'Config for E2.98B (layer-level)', 'Config for E3.18B (layer-level)', 'Config for E3.39B (layer-level)', 'Config for E3.59B (layer-level)', 'Config for E3.79B (layer-level)', 'Config for E2.49B (block-level)', 'Config for E2.73B (block-level)', 'Config for E2.98B (block-level)', 'Config for E3.24B (block-level)', 'Config for E3.49B (block-level)', 'Config for E3.79B (block-level)']\n",
        "\n",
        "def safe_string_to_list(value):\n",
        "    \"\"\"\n",
        "    Converts a string representation of a list into a Python list.\n",
        "    - Converts NaN/missing values to an empty list [].\n",
        "    - Uses eval() to handle expressions like '2_048 * 8'.\n",
        "    - Safely handles non-string values by returning them as is.\n",
        "    \"\"\"\n",
        "    # First, check if the value is missing (NaN, None, etc.)\n",
        "    if isinstance(value, list):\n",
        "        return value\n",
        "\n",
        "    # Priority 2: Now that we know it's not a list, check if it's a missing value.\n",
        "    if pd.isna(value):\n",
        "        return []\n",
        "\n",
        "    # Priority 3: If it's a string, try to evaluate it.\n",
        "    if isinstance(value, str):\n",
        "        try:\n",
        "            return eval(value)\n",
        "        except (SyntaxError, NameError):\n",
        "            return value  # Return invalid string as is\n",
        "\n",
        "    # Fallback for any other type (like an integer)\n",
        "    return value\n",
        "\n",
        "df['FFN Hidden Dims List'] = df['FFN Hidden Dims'].apply(safe_string_to_list)\n",
        "df['Layers Skipped'] = df['Layers Skipped'].apply(safe_string_to_list)\n",
        "\n",
        "df_indexed = df.set_index('name')\n",
        "model_row = df_indexed.loc[config_name]\n",
        "\n",
        "layers_to_skip = model_row['Layers Skipped']\n",
        "ffn_hidden_dims = model_row['FFN Hidden Dims List']\n",
        "ffn_hidden_dims_str = model_row['FFN Hidden Dims']\n",
        "\n",
        "print(config_name)\n",
        "print(\"\\nLayers Skipped:\")\n",
        "print(layers_to_skip)\n",
        "print(\"\\nFFN Hidden Dims:\")\n",
        "pprint(ffn_hidden_dims_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMY2cY6JY15G"
      },
      "source": [
        "If you want to set up your own configuration, please uncomment the following cell and assign values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4Lip1avNY9xd"
      },
      "outputs": [],
      "source": [
        "# Custom config\n",
        "#\n",
        "# layers_to_skip = [] # e.g. [20, 21, 22, 23, 24]\n",
        "# ffn_hidden_dims = [] # e.g. [2048 * 4, ...]\n",
        "# ffn_hidden_dims_str = str(ffn_hidden_dims)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmZcNRV5IUmk"
      },
      "source": [
        "## Slicing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7r0gnOaIWO3"
      },
      "source": [
        "### Load the model config and verify slicing configuration\n",
        "\n",
        "Note: we do not load the model at this stage, just verify that the slicing configuration is possible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6d8a8117e0ca40ae89d94f9968754e59"
          ]
        },
        "id": "O9qsekLDLPiv",
        "outputId": "55d826e6-6168-4aef-f09f-4db0d04b5b58"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "\n",
        "original_config = AutoConfig.from_pretrained(original_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8pgVAGEHA3w4"
      },
      "outputs": [],
      "source": [
        "model_config = original_config.text_config\n",
        "\n",
        "num_layers = model_config.num_hidden_layers\n",
        "final_num_layers = num_layers - len(layers_to_skip)\n",
        "\n",
        "if len(ffn_hidden_dims) != final_num_layers:\n",
        "    raise ValueError(\n",
        "        f\"The length of ffn_hidden_dims ({len(ffn_hidden_dims)}) must be equal \"\n",
        "        f\"to the final number of layers ({final_num_layers}).\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pytVtFyqJPfp"
      },
      "source": [
        "### Update configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kz8KQOEEBERT"
      },
      "outputs": [],
      "source": [
        "num_kv_comp_layers = model_config.num_hidden_layers - model_config.num_kv_shared_layers\n",
        "local_kv_sharing_layer_idx = num_kv_comp_layers - 2\n",
        "global_kv_sharing_layer_idx = num_kv_comp_layers - 1\n",
        "\n",
        "if (local_kv_sharing_layer_idx in layers_to_skip or global_kv_sharing_layer_idx in layers_to_skip):\n",
        "  raise ValueError(f'Layers {local_kv_sharing_layer_idx} and {global_kv_sharing_layer_idx} are reserved.')\n",
        "\n",
        "count_kv_sharing = sum(1 for layer in layers_to_skip if layer >= 20)\n",
        "model_config.num_kv_shared_layers -= count_kv_sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lXEK9hdyBKA1"
      },
      "outputs": [],
      "source": [
        "count_activation_sparsity = sum(1 for layer in layers_to_skip if layer <= 9)\n",
        "activation_sparsity_list = [0.95] * (10 - count_activation_sparsity) + [0] * (\n",
        "    final_num_layers - 10 + count_activation_sparsity\n",
        ")\n",
        "model_config.activation_sparsity_pattern = activation_sparsity_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "urMZZ2SLBOUr"
      },
      "outputs": [],
      "source": [
        "model_config.num_hidden_layers = final_num_layers\n",
        "model_config.intermediate_size = ffn_hidden_dims"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjRMQ97hJFtX"
      },
      "source": [
        "### Save the configuration and the unchanged tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "0e8c4cb7adbe4197b8f781a2110d542d",
            "f0b38a4ccf3c45948a8979a29dd5eb29",
            "842f91a11aad47cda2c798a661d19686",
            "66b118a3a0a743e5b4f8e0db42dde34d",
            "e4edeec981504b5ba326d903e7f31577"
          ]
        },
        "id": "BGCI9MqoBP0w",
        "outputId": "0c8e87b9-8996-4ec1-b360-6a95d7817936"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New config saved to my_modified_gemma_3n_model\n",
            "Final number of layers: 35\n"
          ]
        }
      ],
      "source": [
        "original_config.save_pretrained(local_output_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(original_model_id)\n",
        "tokenizer.save_pretrained(local_output_path)\n",
        "\n",
        "print(f\"New config saved to {local_output_path}\")\n",
        "print(f\"Final number of layers: {model_config.num_hidden_layers}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JlVceVlJVG9"
      },
      "source": [
        "### Load the model checkpoints\n",
        "\n",
        "Note: we are saving the model to disk, so there's no need to have a large CPU/GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "698fd4b0392546c0aecb414d07a4bd61",
            "8c6cd9dc712b48e4a5326828f939d99c",
            "72e223c3fd3942538387393977249db6",
            "f179f9697af948a887556fb84b511b2d",
            "1486ba683c8043c59a3fea1e92c84df2"
          ]
        },
        "id": "5qh_uISmBnVR",
        "outputId": "e6c925e9-2f99-4b5d-b56c-3a687c1cbe1f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 3997.43it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "model_path = snapshot_download(original_model_id, allow_patterns=[\"*.safetensors\"])\n",
        "safetensor_files = [os.path.join(model_path, f) for f in os.listdir(model_path) if f.endswith('.safetensors')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'C:\\\\Users\\\\gabri\\\\.cache\\\\huggingface\\\\hub\\\\models--google--gemma-3n-E4B-it\\\\snapshots\\\\c1221e9c62e34a43ab7ffacd1be0ea71f126ef10'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RHu0up8Jp6n"
      },
      "source": [
        "### Slice the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9ace3835ba99415e9e359472b5f8ef71"
          ]
        },
        "id": "8I2DvFSKB285",
        "outputId": "646e775b-8f6f-45ee-cac4-3ab2e6c86ae6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing shards:   0%|          | 0/4 [00:04<?, ?it/s]\n",
            "Processing shards:  25%|██▌       | 1/4 [00:04<00:14,  4.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving shard model-00001-of-XXXXX.safetensors (size: 7.79 GB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing shards:  75%|███████▌  | 3/4 [04:43<01:31, 91.35s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving shard model-00002-of-XXXXX.safetensors (size: 4.04 GB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing shards: 100%|██████████| 4/4 [05:00<00:00, 75.14s/it]\n"
          ]
        }
      ],
      "source": [
        "from safetensors import safe_open\n",
        "from tqdm.auto import tqdm\n",
        "import re\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "from safetensors.torch import save_file\n",
        "\n",
        "kept_layers_indices = [i for i in range(num_layers) if i not in layers_to_skip]\n",
        "layer_rename_map = {old_idx: new_idx for new_idx, old_idx in enumerate(kept_layers_indices)}\n",
        "\n",
        "# This will store the mapping of tensor names to the file they are saved in\n",
        "weight_map = {}\n",
        "\n",
        "# This will store tensors for the current shard we are building\n",
        "new_shard_state_dict = {}\n",
        "shard_counter = 1\n",
        "total_size = 0\n",
        "\n",
        "pbar = tqdm(total=len(safetensor_files), desc=\"Processing shards\")\n",
        "\n",
        "for shard_path in safetensor_files:\n",
        "    # Open a shard for streaming\n",
        "    with safe_open(shard_path, framework=\"pt\", device=\"cuda\") as f:\n",
        "        # Iterate over each tensor in the shard\n",
        "        for tensor_name in f.keys():\n",
        "            new_tensor_name = tensor_name\n",
        "            tensor = f.get_tensor(tensor_name)\n",
        "\n",
        "            # Case 1: Handle layer-specific parameters\n",
        "            match = re.search(r'\\.layers\\.(\\d+)\\.', tensor_name)\n",
        "            if match:\n",
        "                old_layer_idx = int(match.group(1))\n",
        "\n",
        "                # If this layer is meant to be skipped, we just continue to the next tensor\n",
        "                if old_layer_idx in layers_to_skip:\n",
        "                    continue\n",
        "\n",
        "                # Get the new sequential layer index\n",
        "                new_layer_idx = layer_rename_map[old_layer_idx]\n",
        "                new_tensor_name = tensor_name.replace(\n",
        "                    f'.layers.{old_layer_idx}.',\n",
        "                    f'.layers.{new_layer_idx}.'\n",
        "                )\n",
        "\n",
        "                # Get the target FFN dimension for this new layer\n",
        "                target_ffn_dim = ffn_hidden_dims[new_layer_idx]\n",
        "\n",
        "                # Check if this parameter is part of the FFN and needs slicing\n",
        "                if 'mlp.gate_proj.weight' in new_tensor_name or 'mlp.up_proj.weight' in new_tensor_name:\n",
        "                    # These layers project from model_dim -> ffn_hidden_dim.\n",
        "                    # We slice the output dimension (dim 0).\n",
        "                    tensor = tensor[:target_ffn_dim, :].contiguous()\n",
        "                elif 'mlp.down_proj.weight' in new_tensor_name:\n",
        "                    # This layer projects from ffn_hidden_dim -> model_dim.\n",
        "                    # We slice the input dimension (dim 1).\n",
        "                    tensor = tensor[:, :target_ffn_dim].contiguous()\n",
        "\n",
        "            # Case 2: Handle special non-layer parameters that need slicing\n",
        "            elif 'per_layer_model_projection' in tensor_name:\n",
        "                # Reshape, slice based on kept layers, and reshape back\n",
        "                reshaped_params = tensor.reshape((num_layers, tensor.shape[0] // num_layers, tensor.shape[1]))\n",
        "                tensor = reshaped_params[kept_layers_indices, :, :]\n",
        "                tensor = tensor.reshape(-1, tensor.shape[-1]).contiguous()\n",
        "\n",
        "            elif 'embed_tokens_per_layer' in tensor_name:\n",
        "                # Reshape, slice based on kept layers, and reshape back\n",
        "                reshaped_params = tensor.reshape((tensor.shape[0], num_layers, tensor.shape[1] // num_layers))\n",
        "                tensor = reshaped_params[:, kept_layers_indices, :]\n",
        "                tensor = tensor.reshape(tensor.shape[0], -1).contiguous()\n",
        "\n",
        "            # Add the (potentially modified) tensor to the new shard\n",
        "            new_shard_state_dict[new_tensor_name] = tensor\n",
        "\n",
        "            # Check if the current shard is getting too big\n",
        "            current_shard_size = sum(t.numel() * t.element_size() for t in new_shard_state_dict.values())\n",
        "            if current_shard_size > 4000000000: # Create new shard if current is over 4GB\n",
        "                shard_filename = f\"model-{(shard_counter):05d}-of-XXXXX.safetensors\"\n",
        "                print(f\"Saving shard {shard_filename} (size: {current_shard_size / 1e9:.2f} GB)\")\n",
        "                save_file(new_shard_state_dict, os.path.join(local_output_path, shard_filename), metadata={'format': 'pt'})\n",
        "\n",
        "                # Record which tensors are in this shard\n",
        "                for k in new_shard_state_dict.keys():\n",
        "                    weight_map[k] = os.path.basename(shard_filename)\n",
        "\n",
        "                # Reset for the next shard\n",
        "                shard_counter += 1\n",
        "                new_shard_state_dict = {}\n",
        "                gc.collect() # Free up memory\n",
        "    pbar.update(1)\n",
        "\n",
        "pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FgN1yAQ-CFkE",
        "outputId": "29e8cd9f-1225-4d12-dd14-3bb6f34e809d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving final shard model-00003-of-XXXXX.safetensors\n"
          ]
        }
      ],
      "source": [
        "# Save any remaining tensors in the last shard\n",
        "if new_shard_state_dict:\n",
        "    shard_filename = f\"model-{(shard_counter):05d}-of-XXXXX.safetensors\"\n",
        "    print(f\"Saving final shard {shard_filename}\")\n",
        "    save_file(new_shard_state_dict, os.path.join(local_output_path, shard_filename), metadata={'format': 'pt'})\n",
        "    for k in new_shard_state_dict.keys():\n",
        "        weight_map[k] = os.path.basename(shard_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1ZuSMTulGUPv",
        "outputId": "287493e1-059c-4ef6-cdbf-2d872296c193"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "222"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "del new_shard_state_dict\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wcEWpUKOGO4A",
        "outputId": "08d6d039-1918-4ee6-a92a-c37e525ef788"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 3. Finalizing Model Save ---\n",
            "\n",
            "✅ Model slicing complete. New model saved in: my_modified_gemma_3n_model\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "print(\"\\n--- 3. Finalizing Model Save ---\")\n",
        "\n",
        "# The total number of shards we created\n",
        "num_shards = shard_counter\n",
        "\n",
        "# Update the \"XXXXX\" in the filenames to the correct total number of shards\n",
        "for i in range(1, num_shards + 1):\n",
        "    old_filename = f\"model-{(i):05d}-of-XXXXX.safetensors\"\n",
        "    new_filename = f\"model-{(i):05d}-of-{(num_shards):05d}.safetensors\"\n",
        "\n",
        "    # Rename the file\n",
        "    os.rename(os.path.join(local_output_path, old_filename), os.path.join(local_output_path, new_filename))\n",
        "\n",
        "    # Update the weight_map to point to the new filename\n",
        "    for k, v in weight_map.items():\n",
        "        if v == old_filename:\n",
        "            weight_map[k] = new_filename\n",
        "\n",
        "# Create and save the index.json file\n",
        "index_json = {\n",
        "    \"metadata\": {\n",
        "        \"total_size\": sum(os.path.getsize(os.path.join(local_output_path, f)) for f in os.listdir(local_output_path) if f.endswith('.safetensors'))\n",
        "    },\n",
        "    \"weight_map\": weight_map\n",
        "}\n",
        "\n",
        "with open(os.path.join(local_output_path, \"model.safetensors.index.json\"), \"w\") as f:\n",
        "    json.dump(index_json, f, indent=2)\n",
        "\n",
        "print(f\"\\n✅ Model slicing complete. New model saved in: {local_output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "560b0e55cf384c0488875e972bca52ca"
          ]
        },
        "id": "x1khD_77N8YX",
        "outputId": "3cb8c1e0-98c3-4324-bdad-2da002589975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepended custom description to the model card content.\n",
            "New README.md saved to 'my_modified_gemma_3n_model\\README.md'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gabri\\.cache\\huggingface\\hub\\models--google--gemma-3n-E4B-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import  ModelCard, ModelCardData\n",
        "\n",
        "card = ModelCard.load(original_model_id)\n",
        "card.data.base_model = original_model_id\n",
        "del card.data.extra_gated_heading\n",
        "del card.data.extra_gated_prompt\n",
        "card.data.tags.append(\"matformer\")\n",
        "\n",
        "new_description = f\"\"\"\n",
        "> [!Note]\n",
        "> This is a submodel derived from `{original_model_id}`. It has been modified by slicing specific layers and resizing FFN dimensions. It is not the original model.\n",
        "> To learn more about MatFormers, please review the [launch blog](https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide) and generate your own submodels\n",
        "with the [MatFormer Lab](https://goo.gle/gemma3n-matformer-lab).\n",
        ">\n",
        "\n",
        "Skipped layers: {layers_to_skip}\n",
        "\n",
        "FFN hidden dimensions: {ffn_hidden_dims_str}\n",
        "\"\"\"\n",
        "\n",
        "card.text = new_description + \"\\n\" + card.text\n",
        "print(\"Prepended custom description to the model card content.\")\n",
        "\n",
        "new_readme_path = os.path.join(local_output_path, \"README.md\")\n",
        "card.save(new_readme_path)\n",
        "print(f\"New README.md saved to '{new_readme_path}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuF3U6nhOFwu"
      },
      "source": [
        "## Push the model to Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "9hamNb9aGkjP"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "load_dotenv()\n",
        "\n",
        "TOKEN_WRITE = os.getenv(\"HUGGINGFACE_HUB_TOKEN_WRITE\")\n",
        "\n",
        "login(token=TOKEN_WRITE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating private repository: xValentim/gemma3n-3B-sliced-submodel\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(f\"Creating private repository: {push_hf_repo_id}\")\n",
        "\n",
        "# Instantiate the HfApi client\n",
        "api = HfApi()\n",
        "\n",
        "push_hf_repo_id = \"xValentim/gemma3n-3B-sliced-submodel\"  # @param {type:\"string\"}\n",
        "\n",
        "# Create a new private repository on the Hub.\n",
        "repo_url = api.create_repo(\n",
        "    repo_id=push_hf_repo_id,\n",
        "    private=True,\n",
        "    exist_ok=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OEIMgDQHCUM"
      },
      "outputs": [],
      "source": [
        "print(f\"Uploading files from '{local_output_path}' to '{push_hf_repo_id}'...\")\n",
        "api.upload_folder(\n",
        "    folder_path=local_output_path,\n",
        "    repo_id=push_hf_repo_id,\n",
        "    repo_type=\"model\",\n",
        "    commit_message=\"Upload sliced model checkpoint\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "db433ba38c874916a76d23eb45ff0ac2"
          ]
        },
        "id": "4Gf-3Tg2TaJs",
        "outputId": "7027f15d-6740-426c-e84c-cf063e809526"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "#@title Verify new model can be loaded\n",
        "\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(push_hf_repo_id)# , torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "\n",
        "print(f\"Total Parameters: {model.num_parameters():,}\") # 5,976,833,408\n",
        "print(f\"Total Text Parameters: {model.language_model.num_parameters():,}\") # 4,456,156,768\n",
        "print(f\"Effective Parameters (excluding vision, audio, and Per-Layer-Embeddings): {model.language_model.num_parameters(exclude_embeddings=True):,}\") # 1,905,495,648"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.16it/s]\n",
            "Some parameters are on the meta device because they were offloaded to the disk.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Parameters: 6,843,345,232\n",
            "Total Text Parameters: 5,860,063,728\n",
            "Effective Parameters (excluding vision, audio, and Per-Layer-Embeddings): 2,973,858,288\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./my_modified_gemma_3n_model\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./my_modified_gemma_3n_model\")\n",
        "# processor = AutoProcessor.from_pretrained(\"./my_modified_gemma_3n_model\")\n",
        "\n",
        "print(f\"Total Parameters: {model.num_parameters():,}\") # 5,976,833,408\n",
        "print(f\"Total Text Parameters: {model.language_model.num_parameters():,}\") # 4,456,156,768\n",
        "print(f\"Effective Parameters (excluding vision, audio, and Per-Layer-Embeddings): {model.language_model.num_parameters(exclude_embeddings=True):,}\") # 1,905,495,648"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "GEMMA_PATH = \"./my_modified_gemma_3n_model\" #@param [\"google/gemma-3n-E2B-it\", \"google/gemma-3n-E4B-it\"]\n",
        "# RESOURCE_URL_PREFIX = \"https://raw.githubusercontent.com/google-gemini/gemma-cookbook/refs/heads/main/Demos/sample-data/\"\n",
        "\n",
        "from IPython.display import Audio, Image, Markdown, display\n",
        "\n",
        "class ChatState():\n",
        "  def __init__(self, model, processor):\n",
        "    self.model = model\n",
        "    self.processor = processor\n",
        "    self.history = []\n",
        "\n",
        "  def send_message(self, message, max_tokens=256):\n",
        "    self.history.append(message)\n",
        "\n",
        "    input_ids = self.processor.apply_chat_template(\n",
        "        self.history,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    input_len = input_ids[\"input_ids\"].shape[-1]\n",
        "\n",
        "    input_ids = input_ids.to(self.model.device, dtype=model.dtype)\n",
        "    outputs = self.model.generate(\n",
        "        **input_ids,\n",
        "        max_new_tokens=max_tokens,\n",
        "        disable_compile=True\n",
        "    )\n",
        "    text = self.processor.batch_decode(\n",
        "        outputs[:, input_len:],\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=True\n",
        "    )\n",
        "    self.history.append({\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": text[0]},\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    # display chat\n",
        "    for item in message['content']:\n",
        "      if item['type'] == 'text':\n",
        "        formatted_prompt = \"<font size='+1' color='brown'>🙋‍♂️<blockquote>\\n\" + item['text'] + \"\\n</blockquote></font>\"\n",
        "        display(Markdown(formatted_prompt))\n",
        "      elif item['type'] == 'audio':\n",
        "        display(Audio(item['audio']))\n",
        "      elif item['type'] == 'image':\n",
        "        display(Image(item['image']))\n",
        "\n",
        "    formatted_text = \"<font size='+1' color='teal'>🤖<blockquote>\\n\" + text[0] + \"\\n</blockquote></font>\"\n",
        "    display(Markdown(formatted_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  3.94it/s]\n",
            "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda:0\n",
            "DType: torch.bfloat16\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(GEMMA_PATH)\n",
        "model = AutoModelForImageTextToText.from_pretrained(GEMMA_PATH, torch_dtype=\"auto\", device_map=\"auto\")\n",
        "\n",
        "print(f\"Device: {model.device}\")\n",
        "print(f\"DType: {model.dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s]\n",
            "Some parameters are on the meta device because they were offloaded to the disk.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, Gemma3nForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "model_id = \"./my_modified_gemma_3n_model\"\n",
        "model_id = \"google/gemma-3n-E4B-it\" # @param [\"google/gemma-3n-E4B-it\", \"google/gemma-3n-E4B-pt\"]\n",
        "\n",
        "# Carregando modelo e tokenizer\n",
        "model = Gemma3nForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ").eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Construindo manualmente o chat template\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me a joke about cats.\"}\n",
        "]\n",
        "\n",
        "chat_prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=False  # Aqui queremos o texto para visualizar o prompt\n",
        ")\n",
        "\n",
        "# Tokenizando o prompt manualmente\n",
        "inputs = tokenizer(\n",
        "    chat_prompt,\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cpu\")\n",
        "\n",
        "input_len = inputs[\"input_ids\"].shape[-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Parameters: 7,849,978,192\n",
            "Total Text Parameters: 6,866,696,688\n",
            "Effective Parameters (excluding vision, audio, and Per-Layer-Embeddings): 3,980,491,248\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total Parameters: {model.num_parameters():,}\") # 5,976,833,408\n",
        "print(f\"Total Text Parameters: {model.language_model.num_parameters():,}\") # 4,456,156,768\n",
        "print(f\"Effective Parameters (excluding vision, audio, and Per-Layer-Embeddings): {model.language_model.num_parameters(exclude_embeddings=True):,}\") # 1,905,495,648"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "c:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\transformers\\generation\\utils.py:2505: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generation \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m      3\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      4\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      6\u001b[0m generation \u001b[38;5;241m=\u001b[39m generation[\u001b[38;5;241m0\u001b[39m][input_len:]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Decodificando\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\transformers\\generation\\utils.py:2633\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2625\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2626\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2627\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2628\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2629\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2630\u001b[0m     )\n\u001b[0;32m   2632\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2633\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2634\u001b[0m         input_ids,\n\u001b[0;32m   2635\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2636\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2637\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2638\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2639\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2640\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2641\u001b[0m     )\n\u001b[0;32m   2643\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2644\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[0;32m   2645\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2646\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2647\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2648\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2649\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2650\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\transformers\\generation\\utils.py:3617\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3615\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3616\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3617\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3619\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3620\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3621\u001b[0m     outputs,\n\u001b[0;32m   3622\u001b[0m     model_kwargs,\n\u001b[0;32m   3623\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3624\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\accelerate\\hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\transformers\\utils\\generic.py:961\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    960\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[1;32m--> 961\u001b[0m output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    963\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\transformers\\models\\gemma3n\\modeling_gemma3n.py:2276\u001b[0m, in \u001b[0;36mGemma3nForConditionalGeneration.forward\u001b[1;34m(self, input_ids, pixel_values, input_features, attention_mask, input_features_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, logits_to_keep, **lm_kwargs)\u001b[0m\n\u001b[0;32m   2271\u001b[0m output_attentions \u001b[38;5;241m=\u001b[39m output_attentions \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_attentions\n\u001b[0;32m   2272\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2273\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m   2274\u001b[0m )\n\u001b[1;32m-> 2276\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m   2277\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2278\u001b[0m     pixel_values\u001b[38;5;241m=\u001b[39mpixel_values,\n\u001b[0;32m   2279\u001b[0m     input_features\u001b[38;5;241m=\u001b[39minput_features,\n\u001b[0;32m   2280\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   2281\u001b[0m     input_features_mask\u001b[38;5;241m=\u001b[39minput_features_mask,\n\u001b[0;32m   2282\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   2283\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   2284\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   2285\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m   2286\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   2287\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   2288\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   2289\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   2290\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2291\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2292\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlm_kwargs,\n\u001b[0;32m   2293\u001b[0m )\n\u001b[0;32m   2295\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m   2296\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\transformers\\utils\\generic.py:961\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    960\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[1;32m--> 961\u001b[0m output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    963\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\transformers\\models\\gemma3n\\modeling_gemma3n.py:2034\u001b[0m, in \u001b[0;36mGemma3nModel.forward\u001b[1;34m(self, input_ids, pixel_values, input_features, attention_mask, input_features_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, **lm_kwargs)\u001b[0m\n\u001b[0;32m   2032\u001b[0m per_layer_inputs_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogical_and(input_ids \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, input_ids \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size_per_layer_input)\n\u001b[0;32m   2033\u001b[0m per_layer_inputs_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(per_layer_inputs_mask, input_ids, torch\u001b[38;5;241m.\u001b[39mzeros_like(input_ids))\n\u001b[1;32m-> 2034\u001b[0m per_layer_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_per_layer_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mper_layer_inputs_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2036\u001b[0m \u001b[38;5;66;03m# Handle vision tokens (>= embed_vision.vocab_offset and < embed_audio.vocab_offset)\u001b[39;00m\n\u001b[0;32m   2037\u001b[0m vision_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogical_and(\n\u001b[0;32m   2038\u001b[0m     input_ids \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_vision\u001b[38;5;241m.\u001b[39mvocab_offset, input_ids \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_audio\u001b[38;5;241m.\u001b[39mvocab_offset\n\u001b[0;32m   2039\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\transformers\\models\\gemma3n\\modeling_gemma3n.py:1722\u001b[0m, in \u001b[0;36mGemma3nTextModel.get_per_layer_inputs\u001b[1;34m(self, input_ids)\u001b[0m\n\u001b[0;32m   1721\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_per_layer_inputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m-> 1722\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens_per_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[0;32m   1723\u001b[0m         \u001b[38;5;241m*\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mshape,\n\u001b[0;32m   1724\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers,\n\u001b[0;32m   1725\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size_per_layer_input,\n\u001b[0;32m   1726\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\accelerate\\hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 170\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\accelerate\\hooks.py:341\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[1;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m named_module_tensors(\n\u001b[0;32m    335\u001b[0m     module,\n\u001b[0;32m    336\u001b[0m     include_buffers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffload_buffers,\n\u001b[0;32m    337\u001b[0m     recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_submodules,\n\u001b[0;32m    338\u001b[0m     remove_non_persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    339\u001b[0m ):\n\u001b[0;32m    340\u001b[0m     fp16_statistics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_map\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    343\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mint8:\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\accelerate\\utils\\offload.py:118\u001b[0m, in \u001b[0;36mPrefixedDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\dl\\lib\\site-packages\\accelerate\\utils\\offload.py:171\u001b[0m, in \u001b[0;36mOffloadedWeightsLoader.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(weight_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafetensors_file\u001b[39m\u001b[38;5;124m\"\u001b[39m], framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 171\u001b[0m         tensor \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# if failed to get_tensor on the device, such as bf16 on mps, try to load it on CPU first\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(weight_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafetensors_file\u001b[39m\u001b[38;5;124m\"\u001b[39m], framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "generation = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=False\n",
        ")\n",
        "generation = generation[0][input_len:]\n",
        "\n",
        "# Decodificando\n",
        "decoded = tokenizer.decode(generation, skip_special_tokens=True)\n",
        "print(decoded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO76mJDtOAr7"
      },
      "source": [
        "## Citation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGDUTwIkKrnp"
      },
      "source": [
        "```\n",
        "@article{gemma_3n_2025,\n",
        "    title={Gemma 3n MatFormer Lab},\n",
        "    url={https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/%5BGemma_3n%5DMatFormer_Lab.ipynb},\n",
        "    publisher={Google DeepMind},\n",
        "    author={Puranjay Datta, Aditya Kusupati, Ryan Mullins, Omar Sanseviero, Rakesh Shivanna, Gemma Team},\n",
        "    year={2025}\n",
        "}\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "B7r0gnOaIWO3",
        "pytVtFyqJPfp",
        "pjRMQ97hJFtX",
        "1JlVceVlJVG9",
        "3RHu0up8Jp6n"
      ],
      "name": "[Gemma_3n]MatFormer_Lab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "036afaa2256f446581ef1559d15b555b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_9fcb4212ff934c7084913cec261997f6",
            "placeholder": "​",
            "style": "IPY_MODEL_90e0021b763041309cf7a620160b488c",
            "value": ""
          }
        },
        "06098a3904784a2ba20cb6689419d525": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dae1b6657f04794ac9a0f5265764c9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24c22a54b2f048afade6447b4fe77d0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06098a3904784a2ba20cb6689419d525",
            "placeholder": "​",
            "style": "IPY_MODEL_b8f2ecb8c7b144529b357ab2d243fd6c",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "366cbe589a2b465cbd2a4538060c6b5c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "607aea53773d4220b6137b9b255deadd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68224fc54d0840b7a45af5874d7513d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_366cbe589a2b465cbd2a4538060c6b5c",
            "style": "IPY_MODEL_922dd434870e4fb69e94689112e61c3f",
            "value": true
          }
        },
        "68661aaf7e2441139835d23d80262d0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6df786f15e6547b79375cdc5d831e634": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "70c2df2c30f04e8088111033e6956b6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "7bd5626e14a54280bc6ec225f11cfe6c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90e0021b763041309cf7a620160b488c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "922dd434870e4fb69e94689112e61c3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b7962104b134d618dfd9d8d9a217696": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bd5626e14a54280bc6ec225f11cfe6c",
            "placeholder": "​",
            "style": "IPY_MODEL_68661aaf7e2441139835d23d80262d0a",
            "value": "Connecting..."
          }
        },
        "9fcb4212ff934c7084913cec261997f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a71c0fbff91d4402ae04c52e4e4447ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8f2ecb8c7b144529b357ab2d243fd6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcb3b9b0a6a0405d9394e9859c2cabc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_70c2df2c30f04e8088111033e6956b6d"
          }
        },
        "c3c72e8c65c047e39431efed6889ecdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_607aea53773d4220b6137b9b255deadd",
            "placeholder": "​",
            "style": "IPY_MODEL_0dae1b6657f04794ac9a0f5265764c9c",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "d3ba4b3101de4827a4e58dcc1add1d85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_a71c0fbff91d4402ae04c52e4e4447ca",
            "style": "IPY_MODEL_6df786f15e6547b79375cdc5d831e634",
            "tooltip": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
